[
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Resume",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Zhutong Zhang",
    "section": "",
    "text": "Hello, I am thomson, welcome to my homepage"
  },
  {
    "objectID": "blog/project1/hw1_questions.html",
    "href": "blog/project1/hw1_questions.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a large-scale natural field experiment to test how different fundraising strategies impact charitable giving behavior. In collaboration with a liberal nonprofit organization in the United States, they mailed fundraising letters to more than 50,000 previous donors. These letters were randomly assigned into a control group or one of several treatment groups.\nThe key treatment of interest was the matching grant — a promise from an anonymous donor to match every contribution at a given ratio (1:1, 2:1, or 3:1), up to a certain maximum. Other treatment variations included the maximum size of the matching gift ($25k, $50k, $100k, or unstated) and the suggested donation amount based on the recipient’s past giving history.\nTheir findings, published in the American Economic Review (2007), show that the mere presence of a matching offer significantly increased both donation rates and average revenue per solicitation. However, higher match ratios (e.g., 3:1) did not yield stronger effects than lower ratios (e.g., 1:1), contradicting the assumptions commonly held in fundraising practice. The effect also varied geographically, with stronger results in states that voted Republican in the 2004 election.\nThe article and supporting data are available from the AEA website and from Innovations for Poverty Action via Harvard’s Dataverse.\nThis project seeks to replicate and extend the core results of Karlan and List (2007) using their original dataset and a modern statistical workflow."
  },
  {
    "objectID": "blog/project1/hw1_questions.html#introduction",
    "href": "blog/project1/hw1_questions.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a large-scale natural field experiment to test how different fundraising strategies impact charitable giving behavior. In collaboration with a liberal nonprofit organization in the United States, they mailed fundraising letters to more than 50,000 previous donors. These letters were randomly assigned into a control group or one of several treatment groups.\nThe key treatment of interest was the matching grant — a promise from an anonymous donor to match every contribution at a given ratio (1:1, 2:1, or 3:1), up to a certain maximum. Other treatment variations included the maximum size of the matching gift ($25k, $50k, $100k, or unstated) and the suggested donation amount based on the recipient’s past giving history.\nTheir findings, published in the American Economic Review (2007), show that the mere presence of a matching offer significantly increased both donation rates and average revenue per solicitation. However, higher match ratios (e.g., 3:1) did not yield stronger effects than lower ratios (e.g., 1:1), contradicting the assumptions commonly held in fundraising practice. The effect also varied geographically, with stronger results in states that voted Republican in the 2004 election.\nThe article and supporting data are available from the AEA website and from Innovations for Poverty Action via Harvard’s Dataverse.\nThis project seeks to replicate and extend the core results of Karlan and List (2007) using their original dataset and a modern statistical workflow."
  },
  {
    "objectID": "blog/project1/hw1_questions.html#data",
    "href": "blog/project1/hw1_questions.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\nThis dataset includes 50,083 observations from a large-scale natural field experiment conducted by Karlan and List (2007). Each row represents a previous donor who received a fundraising letter. The data include treatment indicators, demographic variables, prior donation history, and whether or not the individual responded to the solicitation with a gift.\nThe key treatments randomly varied: - Whether a matching grant was offered, - The match ratio (1:1, 2:1, or 3:1), - The maximum size of the matching grant ($25k, $50k, $100k, or unspecified), - The suggested donation amount (based on individual’s prior highest gift).\nBelow is a summary of the main variables used in the analysis:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\n\nimport pandas as pd\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom scipy.stats import ttest_ind\n\ndf = pd.read_stata('karlan_list_2007.dta')\n# Variables to test for balance\nbalance_vars = ['mrm2', 'freq', 'years']  # months since last donation, prior donation freq, years since first donation\n\n# Run t-tests\nt_test_results = {\n    var: ttest_ind(df[df['treatment'] == 1][var].dropna(), df[df['treatment'] == 0][var].dropna())\n    for var in balance_vars\n}\n\n# Run linear regressions: variable ~ treatment\nregression_results = {\n    var: smf.ols(f'{var} ~ treatment', data=df).fit().summary().tables[1]\n    for var in balance_vars\n}\n\nt_test_results, regression_results\n\nprint(\"| Variable | Treatment Coef. | Std. Error | t-stat | p-value |\")\nprint(\"|----------|------------------|------------|--------|---------|\")\n\nfor var in balance_vars:\n    model = smf.ols(f\"{var} ~ treatment\", data=df).fit()\n    coef = model.params['treatment']\n    se = model.bse['treatment']\n    tval = model.tvalues['treatment']\n    pval = model.pvalues['treatment']\n\n    print(f\"| {var:&lt;8} | {coef:.4f}           | {se:.4f}     | {tval:.3f}  | {pval:.4f}  |\")\n\n| Variable | Treatment Coef. | Std. Error | t-stat | p-value |\n|----------|------------------|------------|--------|---------|\n| mrm2     | 0.0137           | 0.1145     | 0.119  | 0.9049  |\n| freq     | -0.0120           | 0.1080     | -0.111  | 0.9117  |\n| years    | -0.0575           | 0.0522     | -1.103  | 0.2700  |\n\n\nAs a check on the randomization mechanism, I tested whether three non-outcome variables differed significantly between the treatment and control groups. Specifically, I examined:\n\nmrm2: Months since the last donation,\nfreq: Number of prior donations,\nyears: Years since the donor’s first contribution.\n\nBoth t-tests and linear regressions were conducted. The results show no statistically significant differences between groups for any of the variables (all p-values &gt; 0.05). For example, the p-value for the difference in mrm2 is 0.905.\nThis confirms what Table 1 of the original paper shows: the random assignment mechanism appears to have worked well, with balanced covariates between the two groups."
  },
  {
    "objectID": "blog/project1/hw1_questions.html#experimental-results",
    "href": "blog/project1/hw1_questions.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\nThe figure below shows the proportion of individuals who made a donation, comparing those in the treatment group (who received a matching offer) versus the control group.\n\nimport pandas as  pd\nimport matplotlib.pyplot as plt\n\ndf = pd.read_stata('karlan_list_2007.dta')\n# Calculate donation rate for treatment and control groups\ndonation_rates = df.groupby('treatment')['gave'].mean()\nlabels = ['Control', 'Treatment']\nrates = [donation_rates[0], donation_rates[1]]\n\n# Create the barplot\nplt.figure(figsize=(6, 4))\nplt.bar(labels, rates)\nplt.ylabel('Proportion Donated')\nplt.title('Donation Rate by Treatment Group')\nplt.ylim(0, max(rates) * 1.2)\nplt.grid(axis='y', linestyle='--', alpha=0.7)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nTo statistically assess whether matched donations increase the likelihood of giving, I ran a t-test comparing the proportion of individuals who donated in the treatment and control groups. The test reveals a significant difference (t = 3.10, p = 0.0019), indicating that individuals who received a matching offer were more likely to contribute.\nTo confirm this result, I also ran a bivariate linear regression where the dependent variable is whether the individual donated (gave), and the independent variable is treatment assignment. The estimated coefficient on treatment is approximately 0.004, meaning that being in the treatment group increases the probability of giving by about 0.4 percentage points. This result is statistically significant at the 1% level.\n\nimport pandas as  pd\nfrom scipy.stats import ttest_ind\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\ndf = pd.read_stata('karlan_list_2007.dta')\n# Split the data into treatment and control groups\ngave_treat = df[df['treatment'] == 1]['gave']\ngave_control = df[df['treatment'] == 0]['gave']\n\n# Run a t-test\nt_stat, p_val = ttest_ind(gave_treat, gave_control)\n\n# Run a bivariate linear regression: gave ~ treatment\nreg_model = smf.ols('gave ~ treatment', data=df).fit()\nreg_summary = reg_model.summary()\n\nt_stat, p_val, reg_summary.tables[1]\n\ncoef = reg_model.params[\"treatment\"]\nse = reg_model.bse[\"treatment\"]\ntval = reg_model.tvalues[\"treatment\"]\npval = reg_model.pvalues[\"treatment\"]\n\nprint(\"\\n| Variable  | Coef.   | Std. Err | t-stat | p-value |\")\nprint(\"|-----------|---------|----------|--------|---------|\")\nprint(f\"| Treatment | {coef:.4f} | {se:.4f}   | {tval:.3f}  | {pval:.4f}  |\")\n\n\n| Variable  | Coef.   | Std. Err | t-stat | p-value |\n|-----------|---------|----------|--------|---------|\n| Treatment | 0.0042 | 0.0013   | 3.101  | 0.0019  |\n\n\nThese findings suggest that even small psychological incentives—like knowing your donation will be matched—can motivate action. Humans are responsive not only to absolute costs and benefits, but also to social cues and perceived multipliers of their impact.\nTo replicate Table 3, column (1) from the original paper, I estimated a probit regression model where the outcome variable is whether a charitable donation was made, and the explanatory variable is treatment assignment. The coefficient on the treatment indicator is 0.0868, statistically significant at the 1% level (p = 0.002). This suggests that being offered a matching grant increases the probability of donation in a statistically significant way, consistent with the authors’ original finding.\n\nimport pandas as pd\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\n# Reload the data\ndata_path = \"karlan_list_2007.dta\"\ndf = pd.read_stata(data_path)\n\n# Run a probit regression: gave ~ treatment\nprobit_model = smf.probit('gave ~ treatment', data=df).fit()\nprobit_summary = probit_model.summary()\n\nprobit_summary.tables[1]\n\nOptimization terminated successfully.\n         Current function value: 0.100443\n         Iterations 7\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n-2.1001\n0.023\n-90.073\n0.000\n-2.146\n-2.054\n\n\ntreatment\n0.0868\n0.028\n3.113\n0.002\n0.032\n0.141\n\n\n\n\n\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\n\nimport pandas as pd\nfrom scipy.stats import ttest_ind\n\ndf = pd.read_stata('karlan_list_2007.dta')\n# Filter only treatment group (exclude control group)\ntreatment_df = df[df['treatment'] == 1]\n# Convert ratio values into clearer string labels for matching\n# Filter for the three match treatment groups\nmatch_df = treatment_df[treatment_df['ratio'].isin([1, 2, 3])].copy()\nmatch_df['ratio_label'] = match_df['ratio'].map({1: '1:1', 2: '2:1', 3: '3:1'})\n\n# Get donation indicators for each ratio\ngave_1to1 = match_df[match_df['ratio_label'] == '1:1']['gave']\ngave_2to1 = match_df[match_df['ratio_label'] == '2:1']['gave']\ngave_3to1 = match_df[match_df['ratio_label'] == '3:1']['gave']\n\n# Run pairwise t-tests again\nttest_2v1 = ttest_ind(gave_2to1, gave_1to1)\nttest_3v1 = ttest_ind(gave_3to1, gave_1to1)\nttest_3v2 = ttest_ind(gave_3to1, gave_2to1)\n\nttest_2v1, ttest_3v1, ttest_3v2\n\nprint(\"| Comparison | t-stat | p-value |\")\nprint(\"|------------|--------|---------|\")\nprint(f\"| 2:1 vs 1:1 | {ttest_2v1.statistic:.3f} | {ttest_2v1.pvalue:.4f} |\")\nprint(f\"| 3:1 vs 1:1 | {ttest_3v1.statistic:.3f} | {ttest_3v1.pvalue:.4f} |\")\nprint(f\"| 3:1 vs 2:1 | {ttest_3v2.statistic:.3f} | {ttest_3v2.pvalue:.4f} |\")\n\n| Comparison | t-stat | p-value |\n|------------|--------|---------|\n| 2:1 vs 1:1 | 0.965 | 0.3345 |\n| 3:1 vs 1:1 | 1.015 | 0.3101 |\n| 3:1 vs 2:1 | 0.050 | 0.9600 |\n\n\nTo assess whether higher match ratios lead to greater likelihood of giving, I compared donation rates across the 1:1, 2:1, and 3:1 match treatments using a series of t-tests.\nThe results reveal no statistically significant differences in giving between any of the match ratios. For example, the comparison between 2:1 and 1:1 yields a p-value of 0.335, and between 3:1 and 1:1 yields a p-value of 0.310. These findings confirm the authors’ statement that while figures suggest a possible trend, “larger match ratios had no additional impact” on donation probability. In short, more generous matching offers do not necessarily increase participation—perhaps because the mere presence of a match, not its size, is what matters psychologically.\nTo complement the earlier t-test analysis, I ran two regression models to estimate the impact of different match ratios on the probability of donation. In the first model, I created dummy variables for 2:1 and 3:1 ratios, using 1:1 as the baseline. In the second model, I used the categorical match ratio variable directly.Both regressions show that the coefficients for 2:1 and 3:1 match ratios are positive but small and not statistically significant (p-values ≈ 0.31–0.34). This confirms the earlier result: increasing the generosity of the match ratio does not meaningfully increase the probability of donation.\nThese results reinforce the idea that it is the presence of a match—not its size—that matters most to potential donors.\nTo complement the t-tests, I assessed the effect of different match ratios on donation probability using regression analysis. First, I created dummy variables for each match ratio:\n\nimport pandas as pd\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\ndf = pd.read_stata('karlan_list_2007.dta')\n# Prepare a clean copy of treatment-only observations with valid ratios\nmatch_df = df[(df['treatment'] == 1) & (df['ratio'].isin([1, 2, 3]))].copy()\n\n# Convert to string for labeling\nmatch_df['ratio_str'] = match_df['ratio'].astype(str)\nmatch_df['ratio_label'] = match_df['ratio_str'].map({'1': '1:1', '2': '2:1', '3': '3:1'})\n\n# Create dummy variables\nmatch_df['ratio1'] = (match_df['ratio_label'] == '1:1').astype(int)\nmatch_df['ratio2'] = (match_df['ratio_label'] == '2:1').astype(int)\nmatch_df['ratio3'] = (match_df['ratio_label'] == '3:1').astype(int)\n\n# Run regression: gave ~ ratio2 + ratio3 (1:1 as baseline)\nreg1 = smf.ols('gave ~ ratio2 + ratio3', data=match_df).fit()\n\n# Alternative: using the ratio_label as a categorical variable\nreg2 = smf.ols('gave ~ C(ratio_label)', data=match_df).fit()\n\nreg1.summary().tables[1], reg2.summary().tables[1]\n\nprint(\"| Variable | Coef. | Std. Err | t-stat | p-value |\")\nprint(\"|----------|-------|----------|--------|---------|\")\n\nfor var in ['ratio2', 'ratio3']:\n    coef = reg1.params[var]\n    se = reg1.bse[var]\n    t = reg1.tvalues[var]\n    p = reg1.pvalues[var]\n    print(f\"| {var:&lt;8} | {coef:.4f} | {se:.4f}   | {t:.3f}  | {p:.4f}  |\")\n\n| Variable | Coef. | Std. Err | t-stat | p-value |\n|----------|-------|----------|--------|---------|\n| ratio2   | 0.0019 | 0.0020   | 0.958  | 0.3383  |\n| ratio3   | 0.0020 | 0.0020   | 1.008  | 0.3133  |\n\n\nmore generous match ratios (2:1, 3:1) do not significantly increase donation rates compared to a standard 1:1 match.\nTo quantify the differences in effectiveness across match ratios, I compared response rates (i.e., donation probability) between 1:1, 2:1, and 3:1 match conditions\n\n# Re-import necessary libraries after kernel reset\nimport pandas as pd\nimport statsmodels.formula.api as smf\n\n# Reload and prepare the data\ndata_path = \"karlan_list_2007.dta\"\ndf = pd.read_stata(data_path)\n\n# Subset to treatment group with valid ratios\nmatch_df = df[(df['treatment'] == 1) & (df['ratio'].isin([1, 2, 3]))].copy()\nmatch_df['ratio_str'] = match_df['ratio'].astype(str)\nmatch_df['ratio_label'] = match_df['ratio_str'].map({'1': '1:1', '2': '2:1', '3': '3:1'})\n\n# Create dummy variables\nmatch_df['ratio1'] = (match_df['ratio_label'] == '1:1').astype(int)\nmatch_df['ratio2'] = (match_df['ratio_label'] == '2:1').astype(int)\nmatch_df['ratio3'] = (match_df['ratio_label'] == '3:1').astype(int)\n\n# Run regression model\nreg1 = smf.ols('gave ~ ratio2 + ratio3', data=match_df).fit()\n\n# Calculate response rates\nresponse_rate_1to1 = match_df[match_df['ratio_label'] == '1:1']['gave'].mean()\nresponse_rate_2to1 = match_df[match_df['ratio_label'] == '2:1']['gave'].mean()\nresponse_rate_3to1 = match_df[match_df['ratio_label'] == '3:1']['gave'].mean()\n\n# Differences in response rates (direct)\ndiff_2v1_direct = response_rate_2to1 - response_rate_1to1\ndiff_3v2_direct = response_rate_3to1 - response_rate_2to1\n\n# Differences in coefficients (model)\ncoef_2v1 = reg1.params['ratio2']\ncoef_3v1 = reg1.params['ratio3']\ndiff_3v2_model = coef_3v1 - coef_2v1\n\nresponse_rate_1to1, response_rate_2to1, response_rate_3to1, diff_2v1_direct, diff_3v2_direct, diff_3v2_model\n\n(0.020749124225276205,\n 0.0226333752469912,\n 0.022733399227244138,\n 0.0018842510217149944,\n 0.00010002398025293902,\n 0.00010002398025350584)\n\n\nUsing the raw data: - The response rate under the 1:1 match was 2.07%, - Under the 2:1 match it was 2.26%, - And under the 3:1 match it was 2.27%.\nThis yields: - A difference of +0.19 percentage points from 1:1 to 2:1, - And a negligible +0.01 percentage points from 2:1 to 3:1.\nThese results are identical to those produced by the fitted coefficients from the linear regression model, reinforcing the idea that higher match ratios do not meaningfully improve response rates.\nThus, the “figures suggest” claim in the original paper is well-supported: increasing the match ratio beyond 1:1 does not substantially increase the likelihood of a donation.\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\n\nimport pandas as pd\nfrom scipy.stats import ttest_ind\nimport statsmodels.formula.api as smf\n\ndf = pd.read_stata('karlan_list_2007.dta')\n# Prepare data\ngave_treat = df[df['treatment'] == 1]['amount']\ngave_control = df[df['treatment'] == 0]['amount']\n\n# T-test on donation amount\nttest_amount = ttest_ind(gave_treat, gave_control)\n\n# Linear regression: amount ~ treatment\nreg_amount = smf.ols('amount ~ treatment', data=df).fit()\n\nttest_amount, reg_amount.summary().tables[1]\n\ncoef = reg_amount.params[\"treatment\"]\nse = reg_amount.bse[\"treatment\"]\ntval = reg_amount.tvalues[\"treatment\"]\npval = reg_amount.pvalues[\"treatment\"]\n\nprint(\"\\n| Variable  | Coef.   | Std. Err | t-stat | p-value |\")\nprint(\"|-----------|---------|----------|--------|---------|\")\nprint(f\"| Treatment | {coef:.4f} | {se:.4f}   | {tval:.3f}  | {pval:.4f}  |\")\n\n\n| Variable  | Coef.   | Std. Err | t-stat | p-value |\n|-----------|---------|----------|--------|---------|\n| Treatment | 0.1536 | 0.0826   | 1.861  | 0.0628  |\n\n\nTo examine whether matched donations influence the size of charitable contributions, I compared the average donation amounts between the treatment and control groups.\nA t-test finds that individuals in the treatment group gave, on average, $0.15 more than those in the control group, but this difference is not statistically significant at the 5% level (p ≈ 0.063). A simple bivariate regression of donation amount on treatment status yields the same estimate and confirms the same level of weak significance.\nThese results suggest that the presence of a match may slightly increase how much people give, but the effect is small and not strongly statistically supported.\nTo further explore the impact of matched donations, I re-ran the analysis on only those individuals who made a donation. This allows us to assess whether the treatment influences how much people give, conditional on deciding to donate.\n\nimport pandas as pd\nfrom scipy.stats import ttest_ind\nimport statsmodels.formula.api as smf\n\ndf = pd.read_stata('karlan_list_2007.dta')\n\n# Filter dataset to only those who made a donation\ndf_positive = df[df['gave'] == 1]\n\n# T-test: amount by treatment group, among donors\nttest_amount_pos = ttest_ind(df_positive[df_positive['treatment'] == 1]['amount'],\n                             df_positive[df_positive['treatment'] == 0]['amount'])\n\n# Regression: amount ~ treatment among donors only\nreg_pos = smf.ols('amount ~ treatment', data=df_positive).fit()\n\nttest_amount_pos, reg_pos.summary().tables[1]\n\n# Format clean markdown table for conditional-on-giving regression\ncoef = reg_pos.params[\"treatment\"]\nse = reg_pos.bse[\"treatment\"]\ntval = reg_pos.tvalues[\"treatment\"]\npval = reg_pos.pvalues[\"treatment\"]\n\nprint(\"\\n| Variable  | Coef.   | Std. Err | t-stat | p-value |\")\nprint(\"|-----------|---------|----------|--------|---------|\")\nprint(f\"| Treatment | {coef:.4f} | {se:.4f}   | {tval:.3f}  | {pval:.4f}  |\")\n\n\n| Variable  | Coef.   | Std. Err | t-stat | p-value |\n|-----------|---------|----------|--------|---------|\n| Treatment | -1.6684 | 2.8724   | -0.581  | 0.5615  |\n\n\nAmong donors, the average contribution was $45.5 in the control group and slightly lower in the treatment group (about $1.67 less), though this difference is not statistically significant (p ≈ 0.56).\nImportantly, the coefficient on treatment does not have a causal interpretation in this context, because we are conditioning on a post-treatment variable (whether a donation was made). This introduces selection bias. The treatment may have influenced both the likelihood of giving and the type of person who decides to give, thus confounding this estimate.\nInterpretation This analysis shows that matched donations do not increase the size of contributions among those who donate. If anything, the treatment group gives slightly less, though the difference is small and statistically insignificant.\nThe plots:\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndf = pd.read_stata('karlan_list_2007.dta')\n# Filter for those who donated\ndonated_df = df[df['gave'] == 1]\n\n# Split into treatment and control\ndonated_treat = donated_df[donated_df['treatment'] == 1]['amount']\ndonated_control = donated_df[donated_df['treatment'] == 0]['amount']\n\n# Compute means\nmean_treat = donated_treat.mean()\nmean_control = donated_control.mean()\n\n# Plot histograms\nfig, axes = plt.subplots(1, 2, figsize=(12, 5), sharey=True)\n\n# Control group\naxes[0].hist(donated_control, bins=30, color='lightblue', edgecolor='black')\naxes[0].axvline(mean_control, color='red', linestyle='dashed', linewidth=2)\naxes[0].set_title('Control Group (Donors Only)')\naxes[0].set_xlabel('Donation Amount ($)')\naxes[0].set_ylabel('Number of Donors')\naxes[0].text(mean_control + 1, axes[0].get_ylim()[1]*0.9,\n             f'Mean = ${mean_control:.2f}', color='red')\n\n# Treatment group\naxes[1].hist(donated_treat, bins=30, color='lightgreen', edgecolor='black')\naxes[1].axvline(mean_treat, color='red', linestyle='dashed', linewidth=2)\naxes[1].set_title('Treatment Group (Donors Only)')\naxes[1].set_xlabel('Donation Amount ($)')\naxes[1].text(mean_treat + 1, axes[1].get_ylim()[1]*0.9,\n             f'Mean = ${mean_treat:.2f}', color='red')\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "blog/project1/hw1_questions.html#simulation-experiment",
    "href": "blog/project1/hw1_questions.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Simulation parameters\nn = 10_000\np_control = 0.018\np_treatment = 0.022\n\n# Simulate 10,000 Bernoulli draws for each group\nsim_control = np.random.binomial(n=1, p=p_control, size=n)\nsim_treatment = np.random.binomial(n=1, p=p_treatment, size=n)\n\n# Compute the difference at each step\ndiffs = sim_treatment - sim_control\n\n# Compute cumulative average of differences\ncumulative_avg = np.cumsum(diffs) / np.arange(1, n + 1)\n\n# True difference in means\ntrue_diff = p_treatment - p_control\n\n# Plot\nplt.figure(figsize=(10, 5))\nplt.plot(cumulative_avg, label='Cumulative Avg. of Differences')\nplt.axhline(true_diff, color='red', linestyle='--', label=f'True Mean Diff = {true_diff:.004f}')\nplt.title('Law of Large Numbers: Convergence of Simulated Mean Difference')\nplt.xlabel('Number of Simulations')\nplt.ylabel('Cumulative Average Difference')\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nTo illustrate the Law of Large Numbers, I simulate 10,000 Bernoulli trials for a control group (p = 0.018) and 10,000 for a treatment group (p = 0.022). Each simulated draw represents whether or not an individual donates.\nI compute the difference in donation outcome for each simulated pair and plot the cumulative average of these differences over time.\nAs the number of simulations increases, the cumulative average converges to the true mean difference (0.004), as expected by the Law of Large Numbers.\nThis helps reinforce the idea that even small differences in probability become reliably detectable with enough observations — the essence of statistical inference using t-tests and p-values.\n\n\nCentral Limit Theorem\nTo illustrate the Central Limit Theorem, I simulate differences in donation probability between a treatment group (p = 0.022) and a control group (p = 0.018). For each sample size (50, 200, 500, 1000), I:\n\nRandomly draw that number of observations from each group,\nCompute the average difference in donation rates,\nRepeat the process 1,000 times,\nPlot the distribution of those 1,000 differences.\n\nEach histogram shows the resulting distribution of sample mean differences. As expected, we observe:\n\nMore spread and noise at smaller sample sizes (e.g., 50),\nTighter, more normal-shaped distributions at larger sizes (e.g., 1000),\nThe true difference in means (0.004) becomes more distinguishable as sample size increases.\n\nImportantly, for small samples (e.g., n = 50), zero lies well within the distribution, suggesting we might fail to detect a difference. But for large samples, zero is pushed toward the tail, indicating a statistically detectable difference. This is the essence of the Central Limit Theorem in practice.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n# Parameters\np_control = 0.018\np_treatment = 0.022\nsample_sizes = [50, 200, 500, 1000]\nnum_reps = 1000\n\n# Storage for distributions of average differences\ndiff_distributions = {}\n\n# Run simulations for each sample size\nfor n in sample_sizes:\n    avg_diffs = []\n    for _ in range(num_reps):\n        sample_control = np.random.binomial(1, p_control, size=n)\n        sample_treatment = np.random.binomial(1, p_treatment, size=n)\n        avg_diff = sample_treatment.mean() - sample_control.mean()\n        avg_diffs.append(avg_diff)\n    diff_distributions[n] = avg_diffs\n\n# Plot histograms\nfig, axes = plt.subplots(2, 2, figsize=(12, 8))\naxes = axes.flatten()\ntrue_diff = p_treatment - p_control\n\nfor i, n in enumerate(sample_sizes):\n    axes[i].hist(diff_distributions[n], bins=30, color='skyblue', edgecolor='black')\n    axes[i].axvline(0, color='black', linestyle='--', label='Zero')\n    axes[i].axvline(true_diff, color='red', linestyle='--', label='True Mean Diff')\n    axes[i].set_title(f'Sample Size = {n}')\n    axes[i].set_xlabel('Avg. Treatment - Control Difference')\n    axes[i].set_ylabel('Frequency')\n    axes[i].legend()\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "hw1_questions (1).html",
    "href": "hw1_questions (1).html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nto do: expand on the description of the experiment.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "hw1_questions (1).html#introduction",
    "href": "hw1_questions (1).html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nto do: expand on the description of the experiment.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "hw1_questions (1).html#data",
    "href": "hw1_questions (1).html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\ntodo: Read the data into R/Python and describe the data\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\ntodo: test a few variables other than the key outcome variables (for example, test months since last donation) to see if the treatment and control groups are statistically significantly different at the 95% confidence level. Do each as a t-test and separately as a linear regression, and confirm you get the exact same results from both methods. When doing a t-test, use the formula in the class slides. When doing the linear regression, regress for example mrm2 on treatment and look at the estimated coefficient on the treatment variable. It might be helpful to compare parts of your analysis to Table 1 in the paper. Be sure to comment on your results (hint: why is Table 1 included in the paper)."
  },
  {
    "objectID": "hw1_questions (1).html#experimental-results",
    "href": "hw1_questions (1).html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\ntodo: make a barplot with two bars. Each bar is the proportion of people who donated. One bar for treatment and one bar for control.\ntodo: run a t-test between the treatment and control groups on the binary outcome of whether any charitable donation was made. Also run a bivariate linear regression that demonstrates the same finding. (It may help to confirm your calculations match Table 2a Panel A.) Report your statistical results and interpret them in the context of the experiment (e.g., if you found a difference with a small p-value or that was statistically significant at some threshold, what have you learned about human behavior? Use mostly English words, not numbers or stats, to explain your finding.)\ntodo: run a probit regression where the outcome variable is whether any charitable donation was made and the explanatory variable is assignment to treatment or control. Confirm that your results replicate Table 3 column 1 in the paper.\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\ntodo: Use a series of t-tests to test whether the size of the match ratio has an effect on whether people donate or not. For example, does the 2:1 match rate lead increase the likelihood that someone donates as compared to the 1:1 match rate? Do your results support the “figures suggest” comment the authors make on page 8?\ntodo: Assess the same issue using a regression. Specifically, create the variable ratio1 then regress gave on ratio1, ratio2, and ratio3 (or alternatively, regress gave on the categorical variable ratio). Interpret the coefficients and their statistical precision.\ntodo: Calculate the response rate difference between the 1:1 and 2:1 match ratios and the 2:1 and 3:1 ratios. Do this directly from the data, and do it by computing the differences in the fitted coefficients of the previous regression. what do you conclude regarding the effectiveness of different sizes of matched donations?\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\ntodo: Calculate a t-test or run a bivariate linear regression of the donation amount on the treatment status. What do we learn from doing this analysis?\ntodo: now limit the data to just people who made a donation and repeat the previous analysis. This regression allows you to analyze how much respondents donate conditional on donating some positive amount. Interpret the regression coefficients – what did we learn? Does the treatment coefficient have a causal interpretation?\ntodo: Make two plot: one for the treatment group and one for the control. Each plot should be a histogram of the donation amounts only among people who donated. Add a red vertical bar or some other annotation to indicate the sample average for each plot."
  },
  {
    "objectID": "hw1_questions (1).html#simulation-experiment",
    "href": "hw1_questions (1).html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\nto do: Make a plot like those on slide 43 from our first class and explain the plot to the reader. To do this, you will simulate 100,00 draws from the control distribution and 10,000 draws from the treatment distribution. You’ll then calculate a vector of 10,000 differences, and then you’ll plot the cumulative average of that vector of differences. Comment on whether the cumulative average approaches the true difference in means.\n\n\nCentral Limit Theorem\nto do: Make 4 histograms like those on slide 44 from our first class at sample sizes 50, 200, 500, and 1000 and explain these plots to the reader. To do this for a sample size of e.g. 50, take 50 draws from each of the control and treatment distributions, and calculate the average difference between those draws. Then repeat that process 999 more times so that you have 1000 averages. Plot the histogram of those averages. Comment on whether zero is in the “middle” of the distribution or whether it’s in the “tail.”"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "My Projects",
    "section": "",
    "text": "Multinomial Logit Model\n\n\n\n\n\n\nZhutong Zhang\n\n\nMay 17, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson Regression Examples\n\n\n\n\n\n\nZhutong Zhang\n\n\nMay 7, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nA Replication of Karlan and List (2007)\n\n\n\n\n\n\nZhutong Zhang\n\n\nApr 17, 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/project2/hw2_questions.html",
    "href": "blog/project2/hw2_questions.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\n\nCode\nimport pandas as pd\n\n# Load the Blueprinty customer data\ndf = pd.read_csv(\"blueprinty.csv\")\n\ndf.head()\n\n\n\n\n\n\n\n\n\npatents\nregion\nage\niscustomer\n\n\n\n\n0\n0\nMidwest\n32.5\n0\n\n\n1\n3\nSouthwest\n37.5\n0\n\n\n2\n4\nNorthwest\n27.0\n1\n\n\n3\n3\nNortheast\n24.5\n0\n\n\n4\n3\nSouthwest\n37.0\n0\n\n\n\n\n\n\n\nCustomers of Blueprinty tend to have a slightly higher number of patents than non-customers. The mean number of patents for non-customers is 3.47, while for customers it is 4.13. The histogram confirms this difference: while both groups are centered around 2 to 5 patents, Blueprinty customers are skewed slightly toward higher patent counts. This suggests a potential relationship worth modeling, though further analysis is needed to control for other factors such as firm age and region.\n\n\nCode\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set(style=\"whitegrid\")\n\n\nmean_patents = df.groupby(\"iscustomer\")[\"patents\"].mean()\n\n\nplt.figure(figsize=(10, 5))\nsns.histplot(data=df, x=\"patents\", hue=\"iscustomer\", bins=20, kde=False, multiple=\"dodge\")\nplt.xlabel(\"Number of Patents\")\nplt.ylabel(\"Count\")\nplt.title(\"Distribution of Patents by Customer Status\")\nplt.legend(title=\"Customer\", labels=[\"Non-customer\", \"Customer\"])\nplt.tight_layout()\nplt.show()\n\nmean_patents\n\n\n\n\n\n\n\n\n\niscustomer\n0    3.473013\n1    4.133056\nName: patents, dtype: float64\n\n\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\n\nCode\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set(style=\"whitegrid\")\n\nplt.figure(figsize=(10, 5))\nsns.boxplot(data=df, x=\"iscustomer\", y=\"age\")\nplt.xticks([0, 1], [\"Non-customer\", \"Customer\"])\nplt.xlabel(\"Customer Status\")\nplt.ylabel(\"Firm Age\")\nplt.title(\"Firm Age Distribution by Customer Status\")\nplt.tight_layout()\nplt.show()\n\nplt.figure(figsize=(10, 5))\nsns.countplot(data=df, x=\"region\", hue=\"iscustomer\")\nplt.xlabel(\"Region\")\nplt.ylabel(\"Count\")\nplt.title(\"Regional Distribution by Customer Status\")\nplt.legend(title=\"Customer\", labels=[\"Non-customer\", \"Customer\"])\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n\ndf.groupby(\"iscustomer\")[\"age\"].mean()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\niscustomer\n0    26.101570\n1    26.900208\nName: age, dtype: float64\n\n\nFinding: There appear to be meaningful differences in both the age and regional distribution of Blueprinty customers compared to non-customers. The average firm age is slightly higher for customers (26.9 years) than for non-customers (26.1 years), though the difference is modest. However, regional patterns are more distinct: customers are disproportionately concentrated in the Northeast, while non-customers are more evenly spread across other regions like the Midwest and Southwest. These patterns suggest that customer status is not randomly assigned and motivate the need to control for firm age and region in subsequent regression modeling.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\n\\[\nf(Y_i \\mid \\lambda_i) = \\frac{e^{-\\lambda_i} \\lambda_i^{Y_i}}{Y_i!}\n\\]\nFor a sample \\(Y_1, Y_2, \\dots, Y_n\\), the log-likelihood is:\n\\[\n\\ell(\\lambda) = \\sum_{i=1}^n \\left( -\\lambda + Y_i \\log \\lambda - \\log(Y_i!) \\right)\n\\]\nThis is the function I maximize in order to estimate the parameters via MLE.\n\n\nCode\nimport numpy as np\nfrom scipy.special import gammaln\n\ndef poisson_log_likelihood(lam, y):\n    y = np.asarray(y)\n    return np.sum(-lam + y * np.log(lam) - gammaln(y + 1))\n\n\n\nthe Log-Likelihood Curve:\n\n\nCode\nimport matplotlib.pyplot as plt\n\ny = df['patents'].values\nlambdas_range = np.linspace(0.1, 20, 200)\nlogliks = [poisson_log_likelihood(lam, y) for lam in lambdas_range]\n\nplt.plot(lambdas_range, logliks)\nplt.xlabel(\"λ (lambda)\")\nplt.ylabel(\"Log-Likelihood\")\nplt.title(\"Log-Likelihood of Poisson Model for Various Lambda\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nlambda_mle is Ybar:\n\\[\n\\hat{\\lambda}_{\\text{MLE}} = \\bar{Y}\n\\]\nlambda_mle:\n\n\nCode\nybar = np.mean(y)\n\nround(ybar,4)\n\n\n3.6847\n\n\nThe MLE by optimizing my likelihood function\n\n\nCode\nfrom scipy.optimize import minimize\n\ndef neg_log_likelihood(params):\n    lam = params[0]\n    return -poisson_log_likelihood(lam, y)\n\nresult = minimize(\n    fun=neg_log_likelihood,\n    x0=[1.0],                     \n    bounds=[(0.001, None)],       \n    method=\"L-BFGS-B\"             \n)\n\nlambda_mle = result.x[0]\n\nround(lambda_mle,4)\n\n\n3.6847\n\n\nThe MLE is the same, the Optimization is successful.\n\n\n\nNext, I extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the age, age_squared, region, and whether the firm is a customer of Blueprinty.\nUpdate:\n\n\n\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport scipy.special as sp\nfrom sklearn.preprocessing import OneHotEncoder\n\ndef poisson_log_likelihood(beta, X, y):\n    beta = np.asarray(beta)\n    lin_pred = X @ beta\n    lambda_ = np.exp(np.clip(lin_pred, -20, 20))\n    return float(-np.sum(y * np.log(lambda_) - lambda_ - gammaln(y + 1)))\n\nX = df[[\"age\", \"region\", \"iscustomer\"]].copy()\nX[\"age_squared\"] = X[\"age\"] ** 2\n\nX_model = pd.get_dummies(X, columns=[\"region\"], drop_first=True)\n\nX_model.insert(0, \"intercept\", 1)\nX_model = X_model.astype(float)\n\nY = df[\"patents\"].values\n\nbeta_init = np.zeros(X_model.shape[1])\nres = minimize(poisson_log_likelihood, beta_init, args=(X_model.values, Y), method='BFGS')\n\nbeta_hat = res.x\ncov_matrix = res.hess_inv\nstandard_errors = np.sqrt(np.diag(cov_matrix))\n\nresults_df = pd.DataFrame({\n    \"Estimate\": beta_hat,\n    \"Std Error\": standard_errors\n}, index=X_model.columns)\n\ndisplay(results_df)\n\n\n\n\n\n\n\n\n\nEstimate\nStd Error\n\n\n\n\nintercept\n-0.509956\n0.193063\n\n\nage\n0.148702\n0.014461\n\n\niscustomer\n0.207600\n0.032937\n\n\nage_squared\n-0.002972\n0.000266\n\n\nregion_Northeast\n0.029159\n0.046774\n\n\nregion_Northwest\n-0.017578\n0.057227\n\n\nregion_South\n0.056567\n0.056245\n\n\nregion_Southwest\n0.050589\n0.049652\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nimport statsmodels.api as sm\n\nmodel = sm.GLM(Y, X_model, family=sm.families.Poisson())\n\nglm_model = model.fit()\n\nglm_model.summary()\n\n\n\nGeneralized Linear Model Regression Results\n\n\nDep. Variable:\ny\nNo. Observations:\n1500\n\n\nModel:\nGLM\nDf Residuals:\n1492\n\n\nModel Family:\nPoisson\nDf Model:\n7\n\n\nLink Function:\nLog\nScale:\n1.0000\n\n\nMethod:\nIRLS\nLog-Likelihood:\n-3258.1\n\n\nDate:\nWed, 07 May 2025\nDeviance:\n2143.3\n\n\nTime:\n14:41:36\nPearson chi2:\n2.07e+03\n\n\nNo. Iterations:\n5\nPseudo R-squ. (CS):\n0.1360\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nintercept\n-0.5089\n0.183\n-2.778\n0.005\n-0.868\n-0.150\n\n\nage\n0.1486\n0.014\n10.716\n0.000\n0.121\n0.176\n\n\niscustomer\n0.2076\n0.031\n6.719\n0.000\n0.147\n0.268\n\n\nage_squared\n-0.0030\n0.000\n-11.513\n0.000\n-0.003\n-0.002\n\n\nregion_Northeast\n0.0292\n0.044\n0.669\n0.504\n-0.056\n0.115\n\n\nregion_Northwest\n-0.0176\n0.054\n-0.327\n0.744\n-0.123\n0.088\n\n\nregion_South\n0.0566\n0.053\n1.074\n0.283\n-0.047\n0.160\n\n\nregion_Southwest\n0.0506\n0.047\n1.072\n0.284\n-0.042\n0.143\n\n\n\n\n\nI confirm the validity of our hand-coded Poisson MLE model by comparing it to the output of statsmodels.GLM() with a Poisson family. The coefficient estimates and standard errors are identical, confirming the correctness of our implementation. In particular, the iscustomer coefficient remains positive and statistically significant, supporting the conclusion that Blueprinty customers tend to receive more patents.\nThe Poisson regression results suggest that Blueprinty’s software is associated with increased patenting success. Specifically, the coefficient on the iscustomer variable is positive and statistically significant, indicating that, all else equal, firms using Blueprinty’s tools tend to receive more patents than non-customers. Because the model uses a log link, the effect is multiplicative: being a customer is associated with approximately 23% more patents on average, as exp(0.208)≈1.23. Additionally, the model reveals a nonlinear relationship between firm age and age_squared are negative and significant, implying that mid-aged firms tend to file more patents than either very young or very old firms. The region variable (region_Northeast) is not statistically significant, suggesting no notable difference in patent activity for firms in the Northeast relative to the base region. Overall, the model supports the hypothesis that using Blueprinty’s software is linked to higher patenting activity, though as with any observational analysis, this result should be interpreted with caution due to potential omitted variable bias.\n\n\n\n\n\nCode\nX_0 = X_model.copy(); \nX_0[\"iscustomer\"] = 0\nX_1 = X_model.copy(); \nX_1[\"iscustomer\"] = 1\n\ny_pred_0 = glm_model.predict(X_0)\ny_pred_1 = glm_model.predict(X_1)\n\navg_diff = np.mean(y_pred_1 - y_pred_0)\n\navg_diff\n\n\n0.7927680710452784\n\n\nTo better interpret the effect of Blueprinty’s software, I simulated a counterfactual experiment using our fitted Poisson regression model. I created two hypothetical datasets: one in which all firms are treated as non-customers (iscustomer = 0), and another where all firms are treated as customers (iscustomer = 1). Then computed predicted patent counts under both scenarios. The average difference between the two predictions was 0.79 patents per firm, suggesting that Blueprinty’s software is associated with nearly one additional patent per firm on average."
  },
  {
    "objectID": "blog/project2/hw2_questions.html#blueprinty-case-study",
    "href": "blog/project2/hw2_questions.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\n\nCode\nimport pandas as pd\n\n# Load the Blueprinty customer data\ndf = pd.read_csv(\"blueprinty.csv\")\n\ndf.head()\n\n\n\n\n\n\n\n\n\npatents\nregion\nage\niscustomer\n\n\n\n\n0\n0\nMidwest\n32.5\n0\n\n\n1\n3\nSouthwest\n37.5\n0\n\n\n2\n4\nNorthwest\n27.0\n1\n\n\n3\n3\nNortheast\n24.5\n0\n\n\n4\n3\nSouthwest\n37.0\n0\n\n\n\n\n\n\n\nCustomers of Blueprinty tend to have a slightly higher number of patents than non-customers. The mean number of patents for non-customers is 3.47, while for customers it is 4.13. The histogram confirms this difference: while both groups are centered around 2 to 5 patents, Blueprinty customers are skewed slightly toward higher patent counts. This suggests a potential relationship worth modeling, though further analysis is needed to control for other factors such as firm age and region.\n\n\nCode\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set(style=\"whitegrid\")\n\n\nmean_patents = df.groupby(\"iscustomer\")[\"patents\"].mean()\n\n\nplt.figure(figsize=(10, 5))\nsns.histplot(data=df, x=\"patents\", hue=\"iscustomer\", bins=20, kde=False, multiple=\"dodge\")\nplt.xlabel(\"Number of Patents\")\nplt.ylabel(\"Count\")\nplt.title(\"Distribution of Patents by Customer Status\")\nplt.legend(title=\"Customer\", labels=[\"Non-customer\", \"Customer\"])\nplt.tight_layout()\nplt.show()\n\nmean_patents\n\n\n\n\n\n\n\n\n\niscustomer\n0    3.473013\n1    4.133056\nName: patents, dtype: float64\n\n\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\n\nCode\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set(style=\"whitegrid\")\n\nplt.figure(figsize=(10, 5))\nsns.boxplot(data=df, x=\"iscustomer\", y=\"age\")\nplt.xticks([0, 1], [\"Non-customer\", \"Customer\"])\nplt.xlabel(\"Customer Status\")\nplt.ylabel(\"Firm Age\")\nplt.title(\"Firm Age Distribution by Customer Status\")\nplt.tight_layout()\nplt.show()\n\nplt.figure(figsize=(10, 5))\nsns.countplot(data=df, x=\"region\", hue=\"iscustomer\")\nplt.xlabel(\"Region\")\nplt.ylabel(\"Count\")\nplt.title(\"Regional Distribution by Customer Status\")\nplt.legend(title=\"Customer\", labels=[\"Non-customer\", \"Customer\"])\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n\ndf.groupby(\"iscustomer\")[\"age\"].mean()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\niscustomer\n0    26.101570\n1    26.900208\nName: age, dtype: float64\n\n\nFinding: There appear to be meaningful differences in both the age and regional distribution of Blueprinty customers compared to non-customers. The average firm age is slightly higher for customers (26.9 years) than for non-customers (26.1 years), though the difference is modest. However, regional patterns are more distinct: customers are disproportionately concentrated in the Northeast, while non-customers are more evenly spread across other regions like the Midwest and Southwest. These patterns suggest that customer status is not randomly assigned and motivate the need to control for firm age and region in subsequent regression modeling.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\n\\[\nf(Y_i \\mid \\lambda_i) = \\frac{e^{-\\lambda_i} \\lambda_i^{Y_i}}{Y_i!}\n\\]\nFor a sample \\(Y_1, Y_2, \\dots, Y_n\\), the log-likelihood is:\n\\[\n\\ell(\\lambda) = \\sum_{i=1}^n \\left( -\\lambda + Y_i \\log \\lambda - \\log(Y_i!) \\right)\n\\]\nThis is the function I maximize in order to estimate the parameters via MLE.\n\n\nCode\nimport numpy as np\nfrom scipy.special import gammaln\n\ndef poisson_log_likelihood(lam, y):\n    y = np.asarray(y)\n    return np.sum(-lam + y * np.log(lam) - gammaln(y + 1))\n\n\n\nthe Log-Likelihood Curve:\n\n\nCode\nimport matplotlib.pyplot as plt\n\ny = df['patents'].values\nlambdas_range = np.linspace(0.1, 20, 200)\nlogliks = [poisson_log_likelihood(lam, y) for lam in lambdas_range]\n\nplt.plot(lambdas_range, logliks)\nplt.xlabel(\"λ (lambda)\")\nplt.ylabel(\"Log-Likelihood\")\nplt.title(\"Log-Likelihood of Poisson Model for Various Lambda\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nlambda_mle is Ybar:\n\\[\n\\hat{\\lambda}_{\\text{MLE}} = \\bar{Y}\n\\]\nlambda_mle:\n\n\nCode\nybar = np.mean(y)\n\nround(ybar,4)\n\n\n3.6847\n\n\nThe MLE by optimizing my likelihood function\n\n\nCode\nfrom scipy.optimize import minimize\n\ndef neg_log_likelihood(params):\n    lam = params[0]\n    return -poisson_log_likelihood(lam, y)\n\nresult = minimize(\n    fun=neg_log_likelihood,\n    x0=[1.0],                     \n    bounds=[(0.001, None)],       \n    method=\"L-BFGS-B\"             \n)\n\nlambda_mle = result.x[0]\n\nround(lambda_mle,4)\n\n\n3.6847\n\n\nThe MLE is the same, the Optimization is successful.\n\n\n\nNext, I extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the age, age_squared, region, and whether the firm is a customer of Blueprinty.\nUpdate:\n\n\n\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport scipy.special as sp\nfrom sklearn.preprocessing import OneHotEncoder\n\ndef poisson_log_likelihood(beta, X, y):\n    beta = np.asarray(beta)\n    lin_pred = X @ beta\n    lambda_ = np.exp(np.clip(lin_pred, -20, 20))\n    return float(-np.sum(y * np.log(lambda_) - lambda_ - gammaln(y + 1)))\n\nX = df[[\"age\", \"region\", \"iscustomer\"]].copy()\nX[\"age_squared\"] = X[\"age\"] ** 2\n\nX_model = pd.get_dummies(X, columns=[\"region\"], drop_first=True)\n\nX_model.insert(0, \"intercept\", 1)\nX_model = X_model.astype(float)\n\nY = df[\"patents\"].values\n\nbeta_init = np.zeros(X_model.shape[1])\nres = minimize(poisson_log_likelihood, beta_init, args=(X_model.values, Y), method='BFGS')\n\nbeta_hat = res.x\ncov_matrix = res.hess_inv\nstandard_errors = np.sqrt(np.diag(cov_matrix))\n\nresults_df = pd.DataFrame({\n    \"Estimate\": beta_hat,\n    \"Std Error\": standard_errors\n}, index=X_model.columns)\n\ndisplay(results_df)\n\n\n\n\n\n\n\n\n\nEstimate\nStd Error\n\n\n\n\nintercept\n-0.509956\n0.193063\n\n\nage\n0.148702\n0.014461\n\n\niscustomer\n0.207600\n0.032937\n\n\nage_squared\n-0.002972\n0.000266\n\n\nregion_Northeast\n0.029159\n0.046774\n\n\nregion_Northwest\n-0.017578\n0.057227\n\n\nregion_South\n0.056567\n0.056245\n\n\nregion_Southwest\n0.050589\n0.049652\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nimport statsmodels.api as sm\n\nmodel = sm.GLM(Y, X_model, family=sm.families.Poisson())\n\nglm_model = model.fit()\n\nglm_model.summary()\n\n\n\nGeneralized Linear Model Regression Results\n\n\nDep. Variable:\ny\nNo. Observations:\n1500\n\n\nModel:\nGLM\nDf Residuals:\n1492\n\n\nModel Family:\nPoisson\nDf Model:\n7\n\n\nLink Function:\nLog\nScale:\n1.0000\n\n\nMethod:\nIRLS\nLog-Likelihood:\n-3258.1\n\n\nDate:\nWed, 07 May 2025\nDeviance:\n2143.3\n\n\nTime:\n14:41:36\nPearson chi2:\n2.07e+03\n\n\nNo. Iterations:\n5\nPseudo R-squ. (CS):\n0.1360\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nintercept\n-0.5089\n0.183\n-2.778\n0.005\n-0.868\n-0.150\n\n\nage\n0.1486\n0.014\n10.716\n0.000\n0.121\n0.176\n\n\niscustomer\n0.2076\n0.031\n6.719\n0.000\n0.147\n0.268\n\n\nage_squared\n-0.0030\n0.000\n-11.513\n0.000\n-0.003\n-0.002\n\n\nregion_Northeast\n0.0292\n0.044\n0.669\n0.504\n-0.056\n0.115\n\n\nregion_Northwest\n-0.0176\n0.054\n-0.327\n0.744\n-0.123\n0.088\n\n\nregion_South\n0.0566\n0.053\n1.074\n0.283\n-0.047\n0.160\n\n\nregion_Southwest\n0.0506\n0.047\n1.072\n0.284\n-0.042\n0.143\n\n\n\n\n\nI confirm the validity of our hand-coded Poisson MLE model by comparing it to the output of statsmodels.GLM() with a Poisson family. The coefficient estimates and standard errors are identical, confirming the correctness of our implementation. In particular, the iscustomer coefficient remains positive and statistically significant, supporting the conclusion that Blueprinty customers tend to receive more patents.\nThe Poisson regression results suggest that Blueprinty’s software is associated with increased patenting success. Specifically, the coefficient on the iscustomer variable is positive and statistically significant, indicating that, all else equal, firms using Blueprinty’s tools tend to receive more patents than non-customers. Because the model uses a log link, the effect is multiplicative: being a customer is associated with approximately 23% more patents on average, as exp(0.208)≈1.23. Additionally, the model reveals a nonlinear relationship between firm age and age_squared are negative and significant, implying that mid-aged firms tend to file more patents than either very young or very old firms. The region variable (region_Northeast) is not statistically significant, suggesting no notable difference in patent activity for firms in the Northeast relative to the base region. Overall, the model supports the hypothesis that using Blueprinty’s software is linked to higher patenting activity, though as with any observational analysis, this result should be interpreted with caution due to potential omitted variable bias.\n\n\n\n\n\nCode\nX_0 = X_model.copy(); \nX_0[\"iscustomer\"] = 0\nX_1 = X_model.copy(); \nX_1[\"iscustomer\"] = 1\n\ny_pred_0 = glm_model.predict(X_0)\ny_pred_1 = glm_model.predict(X_1)\n\navg_diff = np.mean(y_pred_1 - y_pred_0)\n\navg_diff\n\n\n0.7927680710452784\n\n\nTo better interpret the effect of Blueprinty’s software, I simulated a counterfactual experiment using our fitted Poisson regression model. I created two hypothetical datasets: one in which all firms are treated as non-customers (iscustomer = 0), and another where all firms are treated as customers (iscustomer = 1). Then computed predicted patent counts under both scenarios. The average difference between the two predictions was 0.79 patents per firm, suggesting that Blueprinty’s software is associated with nearly one additional patent per firm on average."
  },
  {
    "objectID": "blog/project2/hw2_questions.html#airbnb-case-study",
    "href": "blog/project2/hw2_questions.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\n\n\ndata cleaning\n\n\nCode\nimport pandas as pd\nimport statsmodels.api as sm\nimport numpy as np\n\ndf_airbnb = pd.read_csv(\"airbnb.csv\")\n\ndf_airbnb_clean = df_airbnb.dropna(subset=[\n    \"bathrooms\", \"bedrooms\",\n    \"review_scores_cleanliness\", \"review_scores_location\", \"review_scores_value\"\n])\n\ndf_model = df_airbnb_clean[[\n    \"number_of_reviews\", \"days\", \"room_type\", \"bathrooms\", \"bedrooms\", \"price\",\n    \"review_scores_cleanliness\", \"review_scores_location\", \"review_scores_value\",\n    \"instant_bookable\"\n]]\n\n\n\n\nEDA\n\n\nCode\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\neda_cols = [\n    \"number_of_reviews\", \"days\", \"price\", \"bedrooms\", \"bathrooms\",\n    \"review_scores_cleanliness\", \"review_scores_location\", \"review_scores_value\",\n    \"room_type\", \"instant_bookable\"\n]\ndf_eda = df_airbnb_clean.copy()\n\n\ndf_eda[\"log_reviews\"] = np.log1p(df_eda[\"number_of_reviews\"])\n\n\nnum_cols = [\n    \"days\", \"price\", \"bedrooms\", \"bathrooms\",\n    \"review_scores_cleanliness\", \"review_scores_location\", \"review_scores_value\"\n]\n\nfig, axs = plt.subplots(nrows=4, ncols=2, figsize=(14, 16))\naxs = axs.flatten()\n\nfor i, col in enumerate(num_cols):\n    sns.scatterplot(data=df_eda, x=col, y=\"log_reviews\", alpha=0.3, ax=axs[i])\n    sns.regplot(data=df_eda, x=col, y=\"log_reviews\", scatter=False, ax=axs[i], color=\"red\")\n    axs[i].set_title(f\"{col} vs log(Number of Reviews)\")\n\nsns.boxplot(data=df_eda, x=\"room_type\", y=\"log_reviews\", ax=axs[6])\naxs[6].set_title(\"Room Type vs log(Number of Reviews)\")\n\nsns.boxplot(data=df_eda, x=\"instant_bookable\", y=\"log_reviews\", ax=axs[7])\naxs[7].set_title(\"Instant Bookable vs log(Number of Reviews)\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nContinuous Variables:\nDays active: A strong positive trend is visible—listings that have been on the platform longer tend to accumulate more reviews. This makes intuitive sense, as older listings have had more time to receive bookings and feedback.\nPrice: A weak positive relationship appears, but the spread is large. Some higher-priced listings receive few reviews, suggesting that price alone does not determine popularity.\nBedrooms & Bathrooms: Both show weak or flat relationships with review count. While larger listings may accommodate more guests, they do not consistently receive more reviews.\nReview Scores (Cleanliness, Location, Value): Slight upward trends suggest that better-reviewed listings may receive slightly more bookings. However, these effects are modest and nonlinear.\nCategorical Variables:\nRoom Type: Boxplots show that private rooms and entire homes/apt have similar distributions of review counts, while shared rooms tend to receive fewer reviews on average.\nInstant Bookable: Listings that support instant booking tend to have slightly higher review counts, likely due to reduced friction in the booking process.\n\n\nEncoding Process\n\n\nCode\ndf_encoded = pd.get_dummies(df_model, columns=[\"room_type\", \"instant_bookable\"], drop_first=True)\n\n\n\n\nPoisson Regression Model\n\n\nCode\nX_airbnb = df_encoded.drop(columns=\"number_of_reviews\")\nX_airbnb = sm.add_constant(X_airbnb).astype(np.float64)\ny_airbnb = df_encoded[\"number_of_reviews\"].astype(np.float64).values\n\n\nglm_airbnb = sm.GLM(y_airbnb, X_airbnb, family=sm.families.Poisson())\nglm_airbnb_results = glm_airbnb.fit()\n\n\nglm_airbnb_summary = glm_airbnb_results.summary2().tables[1]\n\nglm_airbnb_summary\n\n\n\n\n\n\n\n\n\nCoef.\nStd.Err.\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\n\n\nconst\n3.498049\n1.609066e-02\n217.396336\n0.000000e+00\n3.466512\n3.529587\n\n\ndays\n0.000051\n3.909218e-07\n129.755337\n0.000000e+00\n0.000050\n0.000051\n\n\nbathrooms\n-0.117704\n3.749225e-03\n-31.394205\n2.427557e-216\n-0.125052\n-0.110356\n\n\nbedrooms\n0.074087\n1.991742e-03\n37.197222\n7.567674e-303\n0.070184\n0.077991\n\n\nprice\n-0.000018\n8.326458e-06\n-2.150886\n3.148517e-02\n-0.000034\n-0.000002\n\n\nreview_scores_cleanliness\n0.113139\n1.496336e-03\n75.610552\n0.000000e+00\n0.110206\n0.116072\n\n\nreview_scores_location\n-0.076899\n1.608903e-03\n-47.796153\n0.000000e+00\n-0.080053\n-0.073746\n\n\nreview_scores_value\n-0.091076\n1.803855e-03\n-50.489904\n0.000000e+00\n-0.094612\n-0.087541\n\n\nroom_type_Private room\n-0.010536\n2.738448e-03\n-3.847467\n1.193451e-04\n-0.015903\n-0.005169\n\n\nroom_type_Shared room\n-0.246337\n8.619793e-03\n-28.578053\n1.259254e-179\n-0.263231\n-0.229442\n\n\ninstant_bookable_t\n0.345850\n2.890138e-03\n119.665624\n0.000000e+00\n0.340186\n0.351515\n\n\n\n\n\n\n\n\n\nPredicted versus Actual Reviews plots\n\n\nCode\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\ndf_model[\"predicted_reviews\"] = glm_airbnb_results.predict(X_airbnb)\ndf_model[\"residuals\"] = df_model[\"number_of_reviews\"] - df_model[\"predicted_reviews\"]\n\n\nplt.figure(figsize=(8, 5))\nsns.scatterplot(x=\"predicted_reviews\", y=\"number_of_reviews\", data=df_model, alpha=0.3)\nplt.plot([0, 600], [0, 600], '--', color=\"gray\")\nplt.xlabel(\"Predicted Number of Reviews\")\nplt.ylabel(\"Actual Number of Reviews\")\nplt.title(\"Predicted vs Actual Reviews\")\nplt.xlim(0, 600)\nplt.ylim(0, 600)\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n/var/folders/kr/2yppgsz96hz36454plvx38_00000gn/T/ipykernel_86862/503929660.py:5: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n/var/folders/kr/2yppgsz96hz36454plvx38_00000gn/T/ipykernel_86862/503929660.py:6: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\n\n\n\n\n\n\n\n\nIn summary, a Poisson regression model was used to analyze the number of Airbnb reviews, treated as a proxy for booking frequency. The results indicate that features such as room type, number of bedrooms, and price are significantly associated with variation in review counts. Listings marked as instant bookable are associated with approximately 35% more reviews, suggesting that ease of booking plays an important role in driving customer engagement. While the model captures general trends effectively, the scatter plot of predicted versus actual values reveals a tendency to underestimate listings with very high review counts. This pattern suggests potential benefits from extending the model to account for overdispersion, such as using a negative binomial specification or incorporating interaction terms."
  },
  {
    "objectID": "blog/project3/hw3_questions.html",
    "href": "blog/project3/hw3_questions.html",
    "title": "Multinomial Logit Model",
    "section": "",
    "text": "This assignment expores two methods for estimating the MNL model: (1) via Maximum Likelihood, and (2) via a Bayesian approach using a Metropolis-Hastings MCMC algorithm."
  },
  {
    "objectID": "blog/project3/hw3_questions.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "href": "blog/project3/hw3_questions.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "title": "Multinomial Logit Model",
    "section": "1. Likelihood for the Multi-nomial Logit (MNL) Model",
    "text": "1. Likelihood for the Multi-nomial Logit (MNL) Model\nSuppose we have \\(i=1,\\ldots,n\\) consumers who each select exactly one product \\(j\\) from a set of \\(J\\) products. The outcome variable is the identity of the product chosen \\(y_i \\in \\{1, \\ldots, J\\}\\) or equivalently a vector of \\(J-1\\) zeros and \\(1\\) one, where the \\(1\\) indicates the selected product. For example, if the third product was chosen out of 3 products, then either \\(y=3\\) or \\(y=(0,0,1)\\) depending on how we want to represent it. Suppose also that we have a vector of data on each product \\(x_j\\) (eg, brand, price, etc.).\nWe model the consumer’s decision as the selection of the product that provides the most utility, and we’ll specify the utility function as a linear function of the product characteristics:\n\\[ U_{ij} = x_j'\\beta + \\epsilon_{ij} \\]\nwhere \\(\\epsilon_{ij}\\) is an i.i.d. extreme value error term.\nThe choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer \\(i\\) chooses product \\(j\\):\n\\[ \\mathbb{P}_i(j) = \\frac{e^{x_j'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} \\]\nFor example, if there are 3 products, the probability that consumer \\(i\\) chooses product 3 is:\n\\[ \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{e^{x_1'\\beta} + e^{x_2'\\beta} + e^{x_3'\\beta}} \\]\nA clever way to write the individual likelihood function for consumer \\(i\\) is the product of the \\(J\\) probabilities, each raised to the power of an indicator variable (\\(\\delta_{ij}\\)) that indicates the chosen product:\n\\[ L_i(\\beta) = \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} = \\mathbb{P}_i(1)^{\\delta_{i1}} \\times \\ldots \\times \\mathbb{P}_i(J)^{\\delta_{iJ}}\\]\nNotice that if the consumer selected product \\(j=3\\), then \\(\\delta_{i3}=1\\) while \\(\\delta_{i1}=\\delta_{i2}=0\\) and the likelihood is:\n\\[ L_i(\\beta) = \\mathbb{P}_i(1)^0 \\times \\mathbb{P}_i(2)^0 \\times \\mathbb{P}_i(3)^1 = \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{\\sum_{k=1}^3e^{x_k'\\beta}} \\]\nThe joint likelihood (across all consumers) is the product of the \\(n\\) individual likelihoods:\n\\[ L_n(\\beta) = \\prod_{i=1}^n L_i(\\beta) = \\prod_{i=1}^n \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} \\]\nAnd the joint log-likelihood function is:\n\\[ \\ell_n(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J \\delta_{ij} \\log(\\mathbb{P}_i(j)) \\]"
  },
  {
    "objectID": "blog/project3/hw3_questions.html#simulate-conjoint-data",
    "href": "blog/project3/hw3_questions.html#simulate-conjoint-data",
    "title": "Multinomial Logit Model",
    "section": "2. Simulate Conjoint Data",
    "text": "2. Simulate Conjoint Data\nWe will simulate data from a conjoint experiment about video content streaming services. We elect to simulate 100 respondents, each completing 10 choice tasks, where they choose from three alternatives per task. For simplicity, there is not a “no choice” option; each simulated respondent must select one of the 3 alternatives.\nEach alternative is a hypothetical streaming offer consistent of three attributes: (1) brand is either Netflix, Amazon Prime, or Hulu; (2) ads can either be part of the experience, or it can be ad-free, and (3) price per month ranges from $4 to $32 in increments of $4.\nThe part-worths (ie, preference weights or beta parameters) for the attribute levels will be 1.0 for Netflix, 0.5 for Amazon Prime (with 0 for Hulu as the reference brand); -0.8 for included adverstisements (0 for ad-free); and -0.1*price so that utility to consumer \\(i\\) for hypothethical streaming service \\(j\\) is\n\\[\nu_{ij} = (1 \\times Netflix_j) + (0.5 \\times Prime_j) + (-0.8*Ads_j) - 0.1\\times Price_j + \\varepsilon_{ij}\n\\]\nwhere the variables are binary indicators and \\(\\varepsilon\\) is Type 1 Extreme Value (ie, Gumble) distributed.\nThe following code provides the simulation of the conjoint data.\n\n\n\n\n\n\nNote\n\n\n\n\n\n# set seed for reproducibility\nset.seed(123)\n\n# define attributes\nbrand &lt;- c(\"N\", \"P\", \"H\") # Netflix, Prime, Hulu\nad &lt;- c(\"Yes\", \"No\")\nprice &lt;- seq(8, 32, by=4)\n\n# generate all possible profiles\nprofiles &lt;- expand.grid(\n    brand = brand,\n    ad = ad,\n    price = price\n)\nm &lt;- nrow(profiles)\n\n# assign part-worth utilities (true parameters)\nb_util &lt;- c(N = 1.0, P = 0.5, H = 0)\na_util &lt;- c(Yes = -0.8, No = 0.0)\np_util &lt;- function(p) -0.1 * p\n\n# number of respondents, choice tasks, and alternatives per task\nn_peeps &lt;- 100\nn_tasks &lt;- 10\nn_alts &lt;- 3\n\n# function to simulate one respondent’s data\nsim_one &lt;- function(id) {\n  \n    datlist &lt;- list()\n    \n    # loop over choice tasks\n    for (t in 1:n_tasks) {\n        \n        # randomly sample 3 alts (better practice would be to use a design)\n        dat &lt;- cbind(resp=id, task=t, profiles[sample(m, size=n_alts), ])\n        \n        # compute deterministic portion of utility\n        dat$v &lt;- b_util[dat$brand] + a_util[dat$ad] + p_util(dat$price) |&gt; round(10)\n        \n        # add Gumbel noise (Type I extreme value)\n        dat$e &lt;- -log(-log(runif(n_alts)))\n        dat$u &lt;- dat$v + dat$e\n        \n        # identify chosen alternative\n        dat$choice &lt;- as.integer(dat$u == max(dat$u))\n        \n        # store task\n        datlist[[t]] &lt;- dat\n    }\n    \n    # combine all tasks for one respondent\n    do.call(rbind, datlist)\n}\n\n# simulate data for all respondents\nconjoint_data &lt;- do.call(rbind, lapply(1:n_peeps, sim_one))\n\n# remove values unobservable to the researcher\nconjoint_data &lt;- conjoint_data[ , c(\"resp\", \"task\", \"brand\", \"ad\", \"price\", \"choice\")]\n\n# clean up\nrm(list=setdiff(ls(), \"conjoint_data\"))\n\n\n\nThis structure ensures that each row corresponds to a product alternative within a task, with all feature values and a flag (choice = 1) indicating whether the alternative was selected. The variables are now ready for use in a multinomial logit likelihood function."
  },
  {
    "objectID": "blog/project3/hw3_questions.html#preparing-the-data-for-estimation",
    "href": "blog/project3/hw3_questions.html#preparing-the-data-for-estimation",
    "title": "Multinomial Logit Model",
    "section": "3. Preparing the Data for Estimation",
    "text": "3. Preparing the Data for Estimation\nThe “hard part” of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer \\(i\\), covariate \\(k\\), and product \\(j\\)) instead of the typical 2 dimensions for cross-sectional regression models (consumer \\(i\\) and covariate \\(k\\)). The fact that each task for each respondent has the same number of alternatives (3) helps. In addition, we need to convert the categorical variables for brand and ads into binary variables.\n\nimport pandas as pd\nimport numpy as np\n\n# Load simulated conjoint data\nconjoint_data = pd.read_csv(\"conjoint_data.csv\")\n\n# One-hot encode categorical variables\nX = pd.get_dummies(conjoint_data, columns=[\"brand\", \"ad\"], drop_first=True)\n\n\n# Convert IDs and choice flag to integer\nfor col in ['resp', 'task', 'choice']:\n    X[col] = X[col].astype(int)\n\n# Add unique choice set identifier\nX['choice_set'] = X['resp'].astype(str) + \"_\" + X['task'].astype(str)\n\n# Sort for readability (optional)\nX = X.sort_values(by=['resp', 'task']).reset_index(drop=True)\n\n# Preview cleaned dataset\nX.head()\n\n\n\n\n\n\n\n\nresp\ntask\nchoice\nprice\nbrand_N\nbrand_P\nad_Yes\nchoice_set\n\n\n\n\n0\n1\n1\n1\n28\nTrue\nFalse\nTrue\n1_1\n\n\n1\n1\n1\n0\n16\nFalse\nFalse\nTrue\n1_1\n\n\n2\n1\n1\n0\n16\nFalse\nTrue\nTrue\n1_1\n\n\n3\n1\n2\n0\n32\nTrue\nFalse\nTrue\n1_2\n\n\n4\n1\n2\n1\n16\nFalse\nTrue\nTrue\n1_2"
  },
  {
    "objectID": "blog/project3/hw3_questions.html#estimation-via-maximum-likelihood",
    "href": "blog/project3/hw3_questions.html#estimation-via-maximum-likelihood",
    "title": "Multinomial Logit Model",
    "section": "4. Estimation via Maximum Likelihood",
    "text": "4. Estimation via Maximum Likelihood\nUsing scipy.optimize.minimize, we estimated the four coefficients of the MNL model via Maximum Likelihood. The signs and magnitudes of the estimates are consistent with the true part-worths used during data simulation. For instance, Netflix has the highest utility weight, followed by Prime, and the presence of ads reduces utility. Price has a negative marginal utility as expected. Standard errors and 95% confidence intervals are derived using the inverse of the approximated Hessian.\n\nimport numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.special import logsumexp\n\nfeatures = ['brand_N', 'brand_P', 'ad_Yes', 'price']\nX_data = X[features].astype(float).values\n\ny_data = X['choice'].astype(int).values\n\ngroup_ids = X['choice_set'].astype(str).values\n\n\ndef neg_log_likelihood(beta, X, y, group_ids):\n    \"\"\"\n    beta: parameter vector (length 4)\n    X: (n_obs, n_features)\n    y: (n_obs,)  binary indicator (1 if chosen)\n    group_ids: choice set ID (same for each set of alternatives)\n    \"\"\"\n    Xb = X @ beta\n    df = pd.DataFrame({'group': group_ids, 'xb': Xb})\n    \n    # Compute log-denominator for each group using logsumexp\n    denom = df.groupby('group')['xb'].transform(lambda x: logsumexp(x))\n    \n    log_probs = Xb - denom\n    ll = np.sum(y * log_probs)\n    \n    return -ll  # negative log-likelihood for minimization\n\n\ninit_params = np.zeros(X_data.shape[1])  # e.g. [0, 0, 0, 0]\n\nresult = minimize(\n    fun=neg_log_likelihood,\n    x0=init_params,\n    args=(X_data, y_data, group_ids),\n    method='BFGS',\n    options={'disp': True}\n)\n\n\nbeta_hat = result.x\nhessian_inv = result.hess_inv  # approx. inverse Hessian\nstandard_errors = np.sqrt(np.diag(hessian_inv))\n\n\nz_95 = 1.96\nlower = beta_hat - z_95 * standard_errors\nupper = beta_hat + z_95 * standard_errors\n\n\nsummary_df = pd.DataFrame({\n    'Coefficient': features,\n    'Estimate': beta_hat,\n    'Std. Error': standard_errors,\n    '95% CI Lower': lower,\n    '95% CI Upper': upper\n})\n\nsummary_df.round(4)\n\nOptimization terminated successfully.\n         Current function value: 879.855368\n         Iterations: 12\n         Function evaluations: 85\n         Gradient evaluations: 17\n\n\n\n\n\n\n\n\n\nCoefficient\nEstimate\nStd. Error\n95% CI Lower\n95% CI Upper\n\n\n\n\n0\nbrand_N\n0.9412\n0.1146\n0.7166\n1.1658\n\n\n1\nbrand_P\n0.5016\n0.1207\n0.2650\n0.7383\n\n\n2\nad_Yes\n-0.7320\n0.0886\n-0.9057\n-0.5583\n\n\n3\nprice\n-0.0995\n0.0064\n-0.1119\n-0.0870"
  },
  {
    "objectID": "blog/project3/hw3_questions.html#estimation-via-bayesian-methods",
    "href": "blog/project3/hw3_questions.html#estimation-via-bayesian-methods",
    "title": "Multinomial Logit Model",
    "section": "5. Estimation via Bayesian Methods",
    "text": "5. Estimation via Bayesian Methods\nWe estimate the parameters of the MNL model using Bayesian methods via a custom Metropolis-Hastings algorithm. The posterior is constructed by combining:\n\nThe log-likelihood from the MNL model (same as used for MLE)\nNormal priors:\n\n\\(\\beta_{\\text{Netflix}}, \\beta_{\\text{Prime}}, \\beta_{\\text{Ads}} \\sim \\mathcal{N}(0, 5^2)\\)\n\\(\\beta_{\\text{price}} \\sim \\mathcal{N}(0, 1^2)\\)\n\n\nWe use a multivariate normal proposal with independent components: - \\(\\mathcal{N}(0, 0.05)\\) for the binary features - \\(\\mathcal{N}(0, 0.005)\\) for the price coefficient\nThe MCMC sampler ran for 11,000 iterations, with a burn-in of 1,000. The acceptance rate was approximately 56%, indicating good mixing.\nPosterior trace plots and histograms show convergence and unimodality. The posterior means and credible intervals align closely with the MLE estimates and the true data-generating process.\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.special import logsumexp\nimport matplotlib.pyplot as plt\n\n# Prepare data\nX_data = X[['brand_N', 'brand_P', 'ad_Yes', 'price']].astype(float).values\ny_data = X['choice'].astype(int).values\ngroup_ids = X['choice_set'].astype(str).values\n\n\n# Log-prior: N(0, 5^2) for binaries; N(0, 1^2) for price\ndef log_prior(beta):\n    lp = -0.5 * (beta[0:3]**2 / 25).sum()  # binary features\n    lp += -0.5 * (beta[3]**2 / 1)          # price\n    return lp\n\n# Posterior = log-likelihood + log-prior\ndef log_posterior(beta):\n    return -neg_log_likelihood(beta, X_data, y_data, group_ids) + log_prior(beta)\n\n\n# Metropolis-Hastings sampler with fixed proposal sd\ndef metropolis_sampler(log_post_fn, start, steps=11000, proposal_sd=None):\n    if proposal_sd is None:\n        proposal_sd = np.array([0.05, 0.05, 0.05, 0.005]) \n\n    draws = np.zeros((steps, len(start)))\n    draws[0] = start\n    current_lp = log_post_fn(start)\n\n    for t in range(1, steps):\n        proposal = draws[t - 1] + np.random.normal(0, proposal_sd)\n        proposal_lp = log_post_fn(proposal)\n        log_accept_ratio = proposal_lp - current_lp\n\n        if np.log(np.random.rand()) &lt; log_accept_ratio:\n            draws[t] = proposal\n            current_lp = proposal_lp\n        else:\n            draws[t] = draws[t - 1]\n\n    return draws\n\n\nnp.random.seed(42)\nstart_beta = np.zeros(4)\n\nsamples = metropolis_sampler(log_posterior, start=start_beta, steps=11000)\nposterior = samples[1000:]  # burn-in\n\n# Acceptance rate\naccepted = np.sum(np.any(samples[1:] != samples[:-1], axis=1))\naccept_rate = accepted / (samples.shape[0] - 1)\nprint(f\"Acceptance rate: {accept_rate:.3f}\")\n\nAcceptance rate: 0.567\n\n\n\nTrace plot of the algorithm & Histogram of the posterior distribution\n\nparam_names = [r\"Netflix\", r\"Prime\", r\"Ads\", r\"Price\"]\n\nfig, axes = plt.subplots(4, 2, figsize=(12, 10))\nfor i in range(4):\n    # Trace plot\n    axes[i, 0].plot(posterior[:, i], linewidth=0.7)\n    axes[i, 0].set_title(f\"Trace plot: {param_names[i]}\")\n    axes[i, 0].set_ylabel(\"Value\")\n    axes[i, 0].grid(alpha=0.3)\n\n    # Histogram\n    axes[i, 1].hist(posterior[:, i], bins=30, density=True, alpha=0.7)\n    axes[i, 1].axvline(posterior[:, i].mean(), color='red', linestyle='--', label=\"Mean\")\n    axes[i, 1].set_title(f\"Posterior histogram: {param_names[i]}\")\n    axes[i, 1].set_xlabel(\"Parameter value\")\n    axes[i, 1].set_ylabel(\"Density\")\n    axes[i, 1].grid(alpha=0.3)\n    axes[i, 1].legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nReport\n\nsummary = pd.DataFrame({\n    'Parameter': ['Netflix', 'Prime', 'Ads', 'Price'],\n    'Mean': posterior.mean(axis=0),\n    'Std': posterior.std(axis=0),\n    '2.5%': np.percentile(posterior, 2.5, axis=0),\n    '97.5%': np.percentile(posterior, 97.5, axis=0)\n}).round(4)\n\nsummary\n\n\n\n\n\n\n\n\nParameter\nMean\nStd\n2.5%\n97.5%\n\n\n\n\n0\nNetflix\n0.9457\n0.1127\n0.7321\n1.1707\n\n\n1\nPrime\n0.5067\n0.1136\n0.2882\n0.7350\n\n\n2\nAds\n-0.7326\n0.0831\n-0.8909\n-0.5668\n\n\n3\nPrice\n-0.0998\n0.0063\n-0.1119\n-0.0873\n\n\n\n\n\n\n\n\n\nComparison and Conclusion\nThe results from both Maximum Likelihood Estimation (MLE) and Bayesian inference (via Metropolis-Hastings) are remarkably consistent across all four parameters. The point estimates are nearly identical, and the credible intervals from the Bayesian method align closely with the confidence intervals from the frequentist approach.\nThis agreement confirms the robustness of the modeling setup and the quality of the simulated data. Bayesian methods offer additional benefits such as full posterior distributions and the ability to quantify uncertainty without relying on asymptotic approximations.\nOverall: - Netflix and Prime have positive utility, with Netflix being stronger. - Ad inclusion decreases utility substantially. - Higher price reduces the likelihood of selection, with the negative effect precisely estimated.\nThe Bayesian method provides a powerful complement to MLE, and both approaches support the same substantive conclusions about consumer preference structure."
  },
  {
    "objectID": "blog/project3/hw3_questions.html#discussion",
    "href": "blog/project3/hw3_questions.html#discussion",
    "title": "Multinomial Logit Model",
    "section": "6. Discussion",
    "text": "6. Discussion\n\nInterpreting Parameter Estimates in Real-World Contexts\nIf we did not simulate the data and instead received this dataset from a real-world conjoint study, we would interpret the estimated coefficients as reflecting underlying consumer preferences inferred from choice behavior.\nThe fact that \\(\\beta_{\\text{Netflix}} &gt; \\beta_{\\text{Prime}}\\) suggests that, holding all else equal (ads and price), consumers derive more utility from Netflix than Prime. This likely reflects a stronger brand perception, broader content library, or past satisfaction with Netflix.\nThe negative sign of \\(\\beta_{\\text{price}}\\) is intuitive and economically meaningful: higher prices reduce the likelihood of a product being chosen, all else equal. The size of this coefficient reflects the marginal disutility per dollar increase.\nSimilarly, the negative coefficient for ads indicates a strong consumer preference for ad-free viewing experiences. This supports strategic decisions to offer premium, ad-free tiers for higher willingness-to-pay segments.\nOverall, even without knowledge of the simulation, the parameter estimates align with expected market dynamics. The model captures realistic trade-offs and can inform pricing, positioning, and product bundling strategies.\n\n\nMoving Toward Hierarchical (Random-Parameter) MNL Models\nTo simulate and estimate a hierarchical or multi-level MNL model, we would need to introduce individual-level heterogeneity in the part-worth utilities. In the standard MNL model, all consumers are assumed to share the same coefficients (fixed \\(\\beta\\)). However, in the real world, consumer preferences vary—some may value price sensitivity more, others may strongly prefer ad-free experiences.\n\nTo simulate hierarchical data:\n\nInstead of drawing one global \\(\\beta\\) for all consumers, we would:\n\nFirst specify a population-level distribution for each coefficient, e.g., \\(\\beta_i \\sim \\mathcal{N}(\\mu, \\Sigma)\\)\nFor each respondent \\(i\\), draw an individual-specific \\(\\beta_i\\) from this distribution\nThen use \\(\\beta_i\\) to simulate their choices across tasks\nThis introduces respondent-level variation in preferences\n\n\n\n\nTo estimate such a model:\n\nWe must infer both:\n\nThe distribution of coefficients (hyperparameters \\(\\mu\\), \\(\\Sigma\\)),\nAnd the individual-level coefficients \\(\\beta_i\\) themselves\n\nThis typically requires Bayesian hierarchical modeling using MCMC (e.g., Gibbs or Hamiltonian Monte Carlo), or frequentist approaches like simulated maximum likelihood\nSoftware such as Stan, PyMC, or Hierarchical Bayes in R (e.g., bayesm) is often used\n\n\n\nWhy use hierarchical models?\n\nThey allow more accurate modeling of preference heterogeneity\nThey provide individual-level estimates (ideal for personalization)\nThey are especially useful when the number of tasks per respondent is small, but the dataset is large overall\n\nIn summary, to move from MNL to hierarchical MNL: - Simulation: Add a prior distribution over \\(\\beta_i\\) - Estimation: Use full Bayesian or simulation-based frequentist methods to recover both population and individual-level parameters"
  }
]