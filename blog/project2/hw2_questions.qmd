---
title: "Poisson Regression Examples"
author: "Zhutong Zhang"
date: May 3,2025
callout-appearance: minimal # this hides the blue "i" icon on .callout-notes
---


## Blueprinty Case Study

### Introduction

Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty's software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty's software and after using it. Unfortunately, such data is not available. 

However, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm's number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty's software. The marketing team would like to use this data to make the claim that firms using Blueprinty's software are more successful in getting their patent applications approved.


### Data
```{python}
import pandas as pd

# Load the Blueprinty customer data
df = pd.read_csv("blueprinty.csv")

# Show basic info and first few rows
df.info(), df.head()

```

Customers of Blueprinty tend to have a slightly higher number of patents than non-customers. The mean number of patents for non-customers is **3.47**, while for customers it is **4.13**. The histogram confirms this difference: while both groups are centered around 2 to 5 patents, Blueprinty customers are skewed slightly toward higher patent counts. This suggests a potential relationship worth modeling, though further analysis is needed to control for other factors such as firm age and region.

```{python}
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns


df = pd.read_csv("blueprinty.csv")


sns.set(style="whitegrid")


mean_patents = df.groupby("iscustomer")["patents"].mean()


plt.figure(figsize=(10, 5))
sns.histplot(data=df, x="patents", hue="iscustomer", bins=20, kde=False, multiple="dodge")
plt.xlabel("Number of Patents")
plt.ylabel("Count")
plt.title("Distribution of Patents by Customer Status")
plt.legend(title="Customer", labels=["Non-customer", "Customer"])
plt.tight_layout()
plt.show()

mean_patents
```


Blueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.


```{python}
import seaborn as sns
import matplotlib.pyplot as plt

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv("blueprinty.csv")

sns.set(style="whitegrid")

plt.figure(figsize=(10, 5))
sns.boxplot(data=df, x="iscustomer", y="age")
plt.xticks([0, 1], ["Non-customer", "Customer"])
plt.xlabel("Customer Status")
plt.ylabel("Firm Age")
plt.title("Firm Age Distribution by Customer Status")
plt.tight_layout()
plt.show()

plt.figure(figsize=(10, 5))
sns.countplot(data=df, x="region", hue="iscustomer")
plt.xlabel("Region")
plt.ylabel("Count")
plt.title("Regional Distribution by Customer Status")
plt.legend(title="Customer", labels=["Non-customer", "Customer"])
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

df.groupby("iscustomer")["age"].mean()
```

Finding: There appear to be meaningful differences in both the age and regional distribution of Blueprinty customers compared to non-customers. The average firm age is slightly higher for customers (26.9 years) than for non-customers (26.1 years), though the difference is modest. However, regional patterns are more distinct: customers are disproportionately concentrated in the Northeast, while non-customers are more evenly spread across other regions like the Midwest and Southwest. These patterns suggest that customer status is not randomly assigned and motivate the need to control for firm age and region in subsequent regression modeling.

### Estimation of Simple Poisson Model

Since our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.

$$
f(Y_i \mid \lambda_i) = \frac{e^{-\lambda_i} \lambda_i^{Y_i}}{Y_i!}
$$

The likelihood function for $n$ independent observations is:

$$
L(\boldsymbol{\beta}) = \prod_{i=1}^{n} \frac{e^{-\lambda_i} \lambda_i^{Y_i}}{Y_i!}, \quad \text{where } \lambda_i = \exp(\mathbf{x}_i^\top \boldsymbol{\beta})
$$

Taking logs gives the log-likelihood:

$$
\log L(\boldsymbol{\beta}) = \sum_{i=1}^{n} \left( -\lambda_i + Y_i \log \lambda_i - \log(Y_i!) \right)
$$

Substituting $\lambda_i = \exp(\mathbf{x}_i^\top \boldsymbol{\beta})$, we get:

$$
\log L(\boldsymbol{\beta}) = \sum_{i=1}^{n} \left( -e^{\mathbf{x}_i^\top \boldsymbol{\beta}} + Y_i \cdot \mathbf{x}_i^\top \boldsymbol{\beta} - \log(Y_i!) \right)
$$

This is the function I maximize in order to estimate the parameters via MLE.

```{python}
import numpy as np

def poisson_log_likelihood(y, lam):
    """Compute the log-likelihood of Poisson model given y and lambda."""
    return np.sum(y * np.log(lam) - lam - np.log(np.math.factorial(y)))
```

I define a function that computes the log-likelihood of the Poisson model given a vector of observed outcomes ùëå and a corresponding vector of predicted means Œª. This function will serve as the core of our MLE implementation.

---

We use the observed number of patents to evaluate the Poisson log-likelihood for a range of values of Œª. The plot shows a clear maximum near the empirical mean of the data. This peak corresponds to the maximum likelihood estimate (MLE) of Œª in a model with no covariates, i.e., a constant rate model.
```{python}
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import scipy.special as sp

df = pd.read_csv("blueprinty.csv")

y_obs = df["patents"].values


def poisson_log_likelihood(y, lam):
    return np.sum(y * np.log(lam) - lam - sp.gammaln(y + 1))

lam_range = np.linspace(0.1, 10, 200)
log_liks = [poisson_log_likelihood(y_obs, lam) for lam in lam_range]

plt.figure(figsize=(8, 5))
plt.plot(lam_range, log_liks, label="Log-Likelihood")
plt.xlabel("Lambda")
plt.ylabel("Log-Likelihood")
plt.title("Poisson Log-Likelihood vs. Lambda")
plt.grid(True)
plt.tight_layout()
plt.show()
```


I used maximum likelihood estimation to fit a Poisson regression model with a binary predictor. The model was estimated using scipy.optimize.minimize() and the BFGS algorithm. The estimated coefficients suggest that Blueprinty customers have a higher expected patent rate than non-customers, though the optimizer did not fully converge (success=False), which may indicate a need for better starting values or tighter optimization criteria.

```{python}
import pandas as pd
import numpy as np
import scipy.special as sp
from scipy.optimize import minimize

df = pd.read_csv("blueprinty.csv")


X = df["iscustomer"].values  # predictor
y = df["patents"].values     # response
X_design = np.column_stack((np.ones_like(X), X))  # Add intercept column


def neg_log_likelihood(beta, X, y):
    lam = np.exp(X @ beta)
    return -np.sum(y * np.log(lam) - lam - sp.gammaln(y + 1))


beta_init = np.array([0.1, 0.1])


result = minimize(neg_log_likelihood, beta_init, args=(X_design, y), method='BFGS')

beta_hat = result.x
result_success = result.success
beta_hat, result_success

```

### Estimation of Poisson Regression Model

Next, I extend our simple Poisson model to a Poisson Regression Model such that $Y_i = \text{Poisson}(\lambda_i)$ where $\lambda_i = \exp(X_i'\beta)$. The interpretation is that the success rate of patent awards is not constant across all firms ($\lambda$) but rather is a function of firm characteristics $X_i$. Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.

Update:
```{python}
import pandas as pd
import numpy as np
import scipy.special as sp
from sklearn.preprocessing import OneHotEncoder

df = pd.read_csv("blueprinty.csv")

df["age_squared"] = df["age"] ** 2

# One-hot encode region
region_dummies = pd.get_dummies(df["region"], prefix="region", drop_first=True)

X_covariates = pd.concat([
    df[["age", "age_squared", "iscustomer"]],
    region_dummies
], axis=1)

X_mat = X_covariates.values
y_vec = df["patents"].values

# update log-likelihood
def poisson_regression_loglik(beta, X, y):
    lam = np.exp(X @ beta)
    return -np.sum(y * np.log(lam) - lam - sp.gammaln(y + 1))
```



### table of coefficients and standard errors:

```{python}
import numpy as np
import pandas as pd
import scipy.special as sp
from scipy.optimize import minimize


df = pd.read_csv("blueprinty.csv")

df["age_std"] = (df["age"] - df["age"].mean()) / df["age"].std()
df["age_squared_std"] = df["age_std"] ** 2


region_dummies = pd.get_dummies(df["region"], prefix="region", drop_first=True)


X_df = pd.concat([df[["age_std", "age_squared_std", "iscustomer"]], region_dummies], axis=1)
X_np = X_df.to_numpy(dtype=np.float64)  
X_np = np.column_stack((np.ones(X_np.shape[0]), X_np))  
y_np = df["patents"].to_numpy(dtype=np.float64)         
beta_init = np.zeros(X_np.shape[1])

def poisson_loglik(beta, X, y):
    beta = np.asarray(beta, dtype=np.float64)
    X = np.asarray(X, dtype=np.float64)
    y = np.asarray(y, dtype=np.float64)
    lin_pred = X @ beta
    lam = np.exp(lin_pred)
    return -np.sum(y * np.log(lam) - lam - sp.gammaln(y + 1))


result = minimize(poisson_loglik, beta_init, args=(X_np, y_np), method="BFGS")
beta_hat = result.x
hessian_inv = result.hess_inv
standard_errors = np.sqrt(np.diag(hessian_inv))

var_names = ["Intercept"] + list(X_df.columns)
coef_table = pd.DataFrame({
    "Coefficient": beta_hat,
    "Std. Error": standard_errors
}, index=var_names)

coef_table
```



I confirm the validity of our hand-coded Poisson MLE model by comparing it to the output of statsmodels.GLM() with a Poisson family. The coefficient estimates and standard errors are identical, confirming the correctness of our implementation. In particular, the iscustomer coefficient remains positive and statistically significant, supporting the conclusion that Blueprinty customers tend to receive more patents.


```{python}
import statsmodels.api as sm
import pandas as pd
import numpy as np

df = pd.read_csv("blueprinty.csv")
df["age_std"] = (df["age"] - df["age"].mean()) / df["age"].std()
df["age_squared_std"] = df["age_std"] ** 2


region_dummies = pd.get_dummies(df["region"], prefix="region", drop_first=True)

X_df = pd.concat([df[["age_std", "age_squared_std", "iscustomer"]], region_dummies], axis=1)
X_df = X_df.astype(np.float64)  
X_df_with_const = sm.add_constant(X_df)  

y_vec = df["patents"].astype(np.float64).values  


glm_poisson = sm.GLM(y_vec, X_df_with_const, family=sm.families.Poisson())
glm_results = glm_poisson.fit()


glm_summary = glm_results.summary2().tables[1]
glm_summary
```


The Poisson regression results suggest that Blueprinty's software is associated with increased patenting success. Specifically, the coefficient on the iscustomer variable is positive and statistically significant, indicating that, all else equal, firms using Blueprinty's tools tend to receive more patents than non-customers. Because the model uses a log link, the effect is multiplicative: being a customer is associated with approximately 23% more patents on average, as exp(0.208)‚âà1.23. Additionally, the model reveals a nonlinear relationship between firm age and patent output‚Äîboth age_std and age_squared_std are negative and significant, implying that mid-aged firms tend to file more patents than either very young or very old firms. The region variable (region_Northeast) is not statistically significant, suggesting no notable difference in patent activity for firms in the Northeast relative to the base region. Overall, the model supports the hypothesis that using Blueprinty's software is linked to higher patenting activity, though as with any observational analysis, this result should be interpreted with caution due to potential omitted variable bias.



```{python}
import numpy as np
import pandas as pd
import statsmodels.api as sm

# Step 1: Read and preprocess the data
df = pd.read_csv("blueprinty.csv")
df["age_std"] = (df["age"] - df["age"].mean()) / df["age"].std()
df["age_squared_std"] = df["age_std"] ** 2
region_dummies = pd.get_dummies(df["region"], prefix="region", drop_first=True)

# Step 2: Build the base design matrix
X_df = pd.concat([df[["age_std", "age_squared_std", "iscustomer"]], region_dummies], axis=1)
X_df = X_df.astype(np.float64)  # Ensure numeric format
y_vec = df["patents"].astype(np.float64).values

# Step 3: Fit the Poisson model using statsmodels
X_df_with_const = sm.add_constant(X_df, has_constant='add')
glm_poisson = sm.GLM(y_vec, X_df_with_const, family=sm.families.Poisson())
glm_results = glm_poisson.fit()

# Step 4: Create counterfactual design matrices
X_df_0 = X_df.copy()
X_df_0["iscustomer"] = 0
X_0 = sm.add_constant(X_df_0, has_constant='add').astype(np.float64)

X_df_1 = X_df.copy()
X_df_1["iscustomer"] = 1
X_1 = sm.add_constant(X_df_1, has_constant='add').astype(np.float64)

# Step 5: Generate predictions under each scenario
beta_hat_glm = glm_results.params.values
y_pred_0 = np.exp(X_0 @ beta_hat_glm)
y_pred_1 = np.exp(X_1 @ beta_hat_glm)

# Step 6: Compute average treatment effect (ATE)
diff = y_pred_1 - y_pred_0
ate = diff.mean()
ate

```

To better interpret the effect of Blueprinty's software, I simulated a counterfactual experiment using our fitted Poisson regression model. I created two hypothetical datasets: one in which all firms are treated as non-customers (iscustomer = 0), and another where all firms are treated as customers (iscustomer = 1). Then computed predicted patent counts under both scenarios. The average difference between the two predictions was 0.79 patents per firm, suggesting that Blueprinty's software is associated with nearly one additional patent per firm on average.

## AirBnB Case Study

### Introduction

AirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City.  The data include the following variables:

:::: {.callout-note collapse="true"}
### Variable Definitions

    - `id` = unique ID number for each unit
    - `last_scraped` = date when information scraped
    - `host_since` = date when host first listed the unit on Airbnb
    - `days` = `last_scraped` - `host_since` = number of days the unit has been listed
    - `room_type` = Entire home/apt., Private room, or Shared room
    - `bathrooms` = number of bathrooms
    - `bedrooms` = number of bedrooms
    - `price` = price per night (dollars)
    - `number_of_reviews` = number of reviews for the unit on Airbnb
    - `review_scores_cleanliness` = a cleanliness score from reviews (1-10)
    - `review_scores_location` = a "quality of location" score from reviews (1-10)
    - `review_scores_value` = a "quality of value" score from reviews (1-10)
    - `instant_bookable` = "t" if instantly bookable, "f" if not

::::


```{python}
import pandas as pd
import statsmodels.api as sm
import numpy as np

df_airbnb = pd.read_csv("airbnb.csv")

df_airbnb_clean = df_airbnb.dropna(subset=[
    "bathrooms", "bedrooms",
    "review_scores_cleanliness", "review_scores_location", "review_scores_value"
])


df_model = df_airbnb_clean[[
    "number_of_reviews", "days", "room_type", "bathrooms", "bedrooms", "price",
    "review_scores_cleanliness", "review_scores_location", "review_scores_value",
    "instant_bookable"
]]


df_encoded = pd.get_dummies(df_model, columns=["room_type", "instant_bookable"], drop_first=True)


X_airbnb = df_encoded.drop(columns="number_of_reviews")
X_airbnb = sm.add_constant(X_airbnb).astype(np.float64)
y_airbnb = df_encoded["number_of_reviews"].astype(np.float64).values


glm_airbnb = sm.GLM(y_airbnb, X_airbnb, family=sm.families.Poisson())
glm_airbnb_results = glm_airbnb.fit()


glm_airbnb_summary = glm_airbnb_results.summary2().tables[1]


import matplotlib.pyplot as plt
import seaborn as sns


df_model["predicted_reviews"] = glm_airbnb_results.predict(X_airbnb)
df_model["residuals"] = df_model["number_of_reviews"] - df_model["predicted_reviews"]


plt.figure(figsize=(8, 5))
sns.scatterplot(x="predicted_reviews", y="number_of_reviews", data=df_model, alpha=0.3)
plt.plot([0, 600], [0, 600], '--', color="gray")  # ÂèÇËÄÉÁ∫ø
plt.xlabel("Predicted Number of Reviews")
plt.ylabel("Actual Number of Reviews")
plt.title("Predicted vs Actual Reviews")
plt.xlim(0, 600)
plt.ylim(0, 600)
plt.grid(True)
plt.tight_layout()
plt.show()

glm_airbnb_summary

```



In summary, a Poisson regression model was used to analyze the number of Airbnb reviews, treated as a proxy for booking frequency. The results indicate that features such as room type, number of bedrooms, and price are significantly associated with variation in review counts. Listings marked as instant bookable are associated with approximately 41% more reviews, suggesting that ease of booking plays an important role in driving customer engagement. While the model captures general trends effectively, the scatter plot of predicted versus actual values reveals a tendency to underestimate listings with very high review counts. This pattern suggests potential benefits from extending the model to account for overdispersion, such as using a negative binomial specification or incorporating interaction terms.

